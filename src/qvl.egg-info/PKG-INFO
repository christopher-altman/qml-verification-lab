Metadata-Version: 2.4
Name: qvl
Version: 0.1.0
Summary: Quantum Machine Learning Verification Laboratory
Author: QVL Contributors
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.20.0
Requires-Dist: matplotlib>=3.3.0
Requires-Dist: pyyaml>=5.4.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: tqdm>=4.60.0; extra == "dev"
Dynamic: license-file

# Quantum Machine Learning Verification Laboratory (QVL)

A reproducible verification harness for Quantum Machine Learning systems that challenges a fundamental assumption: **accuracy is not evidence of learning**.

## The Problem

High test accuracy in quantum machine learning models can arise from multiple mechanisms:

1. **True learning**: The model has discovered generalizable patterns
2. **Noise exploitation**: The model exploits specific noise signatures rather than signal
3. **Identifiability collapse**: Parameters are not uniquely determined by data

Standard ML evaluation cannot distinguish between these cases. A model can achieve high accuracy while being fundamentally non-identifiable, making it scientifically invalid and practically unreliable.

## The Phenomenon

In noisy quantum systems, we observe a troubling pattern:

- Models maintain high accuracy under noise
- But the parameter-to-output mapping becomes many-to-one
- Fisher information geometry collapses
- Robustness to seed/initialization vanishes
- The model appears to "work" but is not actually learning

This is not a bug. It's a fundamental challenge for QML verification.

## Our Hypothesis

We hypothesize that QML verification requires a battery of diagnostics beyond accuracy:

**Identifiability**: Can we uniquely recover parameters from observations?
- Fisher information matrix properties
- Parameter space geometry
- Effective dimensionality

**Information Geometry**: What does the loss landscape reveal?
- Hessian eigenspectrum
- Condition numbers
- Local curvature

**Robustness**: Is the solution stable?
- Seed sensitivity
- Noise perturbation response
- Initialization dependence

## Method

QVL implements a systematic verification protocol:

1. **Controlled noise injection** at multiple levels (depolarizing, measurement, amplitude)
2. **Battery metrics** computed on every run (identifiability proxies, Fisher diagnostics, Hessian analysis)
3. **Sweep automation** over noise × seed grids
4. **Stable artifact contracts** for reproducibility
5. **Verification warnings** when accuracy and identifiability diverge

### Noise Parameters

- `depolarizing_p`: Feature noise intensity (simulates decoherence)
- `measurement_bitflip_p`: Label noise (simulates measurement errors)
- `amplitude_gamma`: Regularization strength (simulates amplitude damping)

### Verification Metrics

| Metric | Purpose | Warning Threshold |
|--------|---------|-------------------|
| `accuracy` | Standard performance | N/A |
| `ident_proxy` | Identifiability signal | < 0.1 with accuracy > 0.7 |
| `fisher_condition_number` | Parameter determinability | > 1000 |
| `fisher_effective_rank` | Active parameter dimensions | < 0.5 × theoretical |
| `hessian_min_abs` / `hessian_max_abs` | Loss landscape geometry | Ratio > 1000 |
| `seed_robustness` | Stability across seeds | Variance > 0.1 |

## Implementation

### Architecture

```
qvl/
├── cli.py              # Argument parsing and command dispatch
├── config.py           # YAML loading and validation
├── runner.py           # Single run + sweep orchestration
├── artifacts.py        # Standardized output contract
├── plotting.py         # Hero figures (dark/light variants)
├── batteries/          # Verification metric implementations
└── backends/           # Pluggable experiment backends
    └── toy/            # Toy logistic regression (Prompt A)
```

### Artifact Contract

Every run produces a stable directory structure:

```
artifacts/<experiment_id>/run_<timestamp>_<hash>/
├── config.resolved.json          # Full resolved configuration
├── summary.json                  # Standardized metrics (stable schema)
├── results.jsonl                 # Per-point results (for sweeps)
├── env.json                      # Environment snapshot
├── git.json                      # Git metadata (optional)
├── tables/
│   └── leaderboard.csv
└── figures/
    ├── hero_identifiability_dark.png
    ├── hero_identifiability_light.png
    ├── hero_identifiability_dark_transparent.png
    └── hero_identifiability_light_transparent.png
```

The `summary.json` schema is **guaranteed stable** for downstream automation.

### Backend Plugin System

Backends implement a simple interface:

```python
class Backend:
    def __init__(self, config: dict): ...
    def run(self) -> dict: ...  # Returns metrics, noise, timing
```

Current backends:
- `toy`: Synthetic classification with controlled noise (Prompt A baseline)

Future backends (Prompts B-D):
- `pennylane`: Real QML circuits
- `qiskit`: IBM quantum simulators
- `cirq`: Google quantum frameworks

## Usage

### Installation

```bash
git clone https://github.com/your-org/qml-verification-lab.git
cd qml-verification-lab
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
```

### Quick Start

```bash
# Single run
python -m qvl run --config examples/toy_smoke.yaml --output-dir artifacts/

# Parameter sweep
python -m qvl sweep --config examples/toy_sweep_small.yaml --output-dir artifacts/ --seeds 0,1,2

# Help
python -m qvl --help
```

### Example Configuration

```yaml
experiment_id: my_verification_run
backend: toy
task: classification

training:
  n_samples: 200
  n_features: 2
  n_epochs: 150
  learning_rate: 0.1

noise:
  depolarizing_p: 0.1
  measurement_bitflip_p: 0.05
  amplitude_gamma: 0.01

# For sweeps
sweep:
  depolarizing_p: [0.0, 0.05, 0.1, 0.15, 0.2]
  measurement_bitflip_p: [0.0, 0.05, 0.1, 0.15, 0.2]
```

## Initial Results (Toy Backend)

Even in the toy backend, we observe the core phenomenon:

- At `depolarizing_p = 0.0, measurement_bitflip_p = 0.0`: accuracy ≈ 0.95, identifiability ≈ 0.8
- At `depolarizing_p = 0.2, measurement_bitflip_p = 0.2`: accuracy ≈ 0.70, identifiability ≈ 0.05

**Verification warning triggered**: "High accuracy but low identifiability - potential overfitting or noise dominance"

This demonstrates that noise can degrade identifiability faster than accuracy, invalidating the learned model despite acceptable test performance.

## Interpretation

### Why Accuracy Alone is Insufficient

Traditional ML assumes:
- Unique parameter-to-output mapping (identifiability)
- Smooth, well-conditioned loss landscape
- Stability across random initialization

Quantum systems violate all three under noise. A high-accuracy QML model might be:
- Non-identifiable (many parameter settings produce same output)
- Ill-conditioned (Fisher information matrix near-singular)
- Non-robust (different seeds give different solutions)

These failures are invisible to accuracy metrics but fatal to scientific validity.

### The Verification Gap

The gap between accuracy and identifiability reveals the **verification gap**: the space where a model appears to work but is not actually learning. QVL makes this gap quantifiable and reproducible.

## Development Roadmap

- **Prompt A** (current): MVP with toy backend, stable artifacts, hero plots
- **Prompt B**: Real Fisher information, Hessian computation, PennyLane backend
- **Prompt C**: Extended noise models, robustness batteries, cross-seed analysis
- **Prompt D**: Interactive reports, web dashboard, publication-ready figures

## Citation

If you use QVL in your research, please cite:

```bibtex
@software{qvl2026,
  title={Quantum Machine Learning Verification Laboratory},
  author={QVL Contributors},
  year={2026},
  url={https://github.com/your-org/qml-verification-lab}
}
```

## License

MIT License. See [LICENSE](LICENSE) for details.

## Tags

`qml` · `verification` · `identifiability` · `fisher-information` · `robustness` · `quantum-computing` · `machine-learning` · `reproducibility` · `harness` · `noise-analysis`
